---
title: "Data Analysis Using Statistical Packages: Regression Analysis"
author: "Sho Kuroda / 黒田翔"
date: '最終更新：2025年6月'
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    self_contained: true
  pdf_document:
    toc: yes
# runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

このページに対応するRmdファイル：[GitHub](https://github.com/kurodaecon/dasp/blob/main/rmd/regression.Rmd)

# Data 

このページで使用するデータは，カーセンサーnetという中古車情報サイトに掲載されている [人気車種ランキング](https://www.carsensor.net/ranking/pop/) Top 80 の車両に関する車種レベルの属性．

- ランキングはカーセンサーに掲載されている自動車への問い合わせデータに基づいて車種別に作成されている．
    - このページでは，車種とは「プリウス」（トヨタのハイブリッド車）とか「N-BOX」（ホンダのトールワゴン型軽自動車）のようなカテゴリーを指す語として用いる．
- 一般的に一つの車種には様々なバリエーションがあるが，そのうち一つを選択しその車種を代表する属性とした．
    - たとえば，トヨタのプリウスを例にとると，まず世代と呼ばれる区分けがあり（初代 1997～，2代目 2003～，3代目 2009～，4代目 2015～），さらに「Z」・「G」・「X」・「U」など幾つかのグレードに分かれ，そこからさらにハイブリッドかプラグインハイブリッドか，駆動方式が前輪駆動（FF）か四輪駆動（4WD）か，などの基準で枝分かれする．
    - 幾つかあるバリエーションのうちデータ作成者（黒田）がいずれを選択したか（の規則性）は分析結果に影響を及ぼすが，この演習ではそれは気にしないことにしよう．
- 2025年6月のランキングに基づいてデータを作成した．データセットにランクを表す列はないが，ランキングのとおり昇順で並んでいる（列名を表す行を除いて1行目が1位で，80行目が80位）．
- 変数
    - `model`: 車種
    - `maker`: 製造メーカー
    - `year`: 世代の開始年（一つ前の世代からモデルチェンジした年）［西暦］
    - `price`: 新車時価格（税込）［万円］
    - `seat`: 乗車定員
    - `length`: 全長［m］
    - `width`: 全幅［m］
    - `height`: 全高［m］
    - `weight`: 車両重量［kg］
    - `WLTC`: WLTCモード燃費［km/L］
    - `ps`: 最高出力［ps］ （注：ドイツ語の Pferdestärke の略で馬力の意．1 PS ≒ 0.74 kW）
    - `disp`: 排気量［cc］
    - `hybrid`: ハイブリッド車ダミー
    - `electric`: 電気自動車ダミー

```{r read_data}
car <- read.csv("https://raw.githubusercontent.com/kurodaecon/dasp/refs/heads/main/data/carsensor_ranking_of_most_popular_car_models_asof202506.csv")
str(car)
```

## Summary stat 

長くなるのでこのページには結果を出力しない設定にしているが，実際にはこれを見ながらデータの分布と特徴を理解するのもデータ分析の重要な工程．

```{r summary_stat, eval = FALSE}
summary(car[, c(-1, -2)])
table(car$maker)
hist(car$price)
hist(car$price[car$price <= 1000])
table(car$seat)
table(car$hybrid)
table(car$electric)
round(cor(car[, c(-1, -2, -14)], use = "complete.obs"), 2)
plot(car[, c("price", "weight", "height", "WLTC", "ps", "year")])
```

## Create variables 

```{r create_vars}
car$accel <- car$ps / car$weight  # 加速性能の大まかな指標 (power-to-weight ratio) 
car$kei <- 1*(car$disp <= 660)  # 軽自動車ダミー
```

## Trim dataset 

次の車両は除外する：乗車定員が3以下（スポーツカー・軽トラック等）または6以上（ミニバン・ワゴン車），海外メーカー，1,000万円を超える自動車，電気自動車，

```{r trim_data}
car <- car[car$seat %in% 4:5, ]
car <- car[!(car$maker %in% c("BMW", "Jeep", "Mercedes-Benz", "Porsche")), ]
car <- car[car$price <= 1000, ]
car <- car[car$electric == 0, ]
car <- car[, -14]  # remove "electric" variable 
nrow(car)  # sample size
```


# Simple regression analysis / 単回帰分析

1つの説明変数をもつ次のような回帰モデル（単回帰モデル）を考える．

$$ \mbox{Price} = \alpha + \beta \ \mbox{Weight} + u $$

```{r xy_scatter}
plot(price ~ weight, car, cex = 1.5, xlab = "Curb Weight [kg]", ylab = "Price [10,000 JPY]")
```

上の `plot` 関数は `plot(formula = y ~ x, data = dataset)` の形式で変数を指定しており，`plot(x = dataset$x, y = dataset$y)` と同じ意味．

## Estimation with `lm` function 

`lm` (linear model) 関数で推定する．

```{r reg_lm}
lm(formula = price ~ weight, data = car)
```

次のように `formula =` や `data =` を省略してもよい（出力は省略）．

```{r reg_lm2, eval = FALSE}
lm(price ~ weight, car)
```

以上の推定結果は以下のように回帰式が推定されたことを意味している．

$$ \mbox{Price} = -170 + 0.37 \ \mbox{Weight} + u $$

`weight` の回帰係数は「`weight` が1単位高いと `price` は 0.37 単位高い傾向にある」＝「車両重量が 1kg 高い車の価格は3,700円高い」と解釈される．

- ある特定の車種について「設計を変更して `Weight` を1単位引き上げると `Price` を 0.37 単位引き上げることができる（市場適正価格が3,700円上昇する）」あるいは「政策的介入によって重量が1kg増加したら均衡価格が3,700円上昇する」などのように解釈することはできない．この回帰係数はあくまで両変数の相関的な関係を記述しているだけであり（注：相関係数とは異なる），因果関係を示すものではない．

説明変数が1つの場合，回帰式は下図のように散布図に書き足すことができる．
大雑把に言えば，回帰式の推定（＝回帰係数パラメタを計算すること）とはデータに当てはまる直線の切片 $\alpha$ と傾き $\beta$ を決める手続きである．

```{r reg_lm_scatter}
plot(price ~ weight, car, xlim = c(-100, 2100), ylim = c(-200, 800))
abline(v = 0, col = "grey")
abline(lm(price ~ weight, car), col = "red")
points(0, -169.66, pch = 15, col = "red")
text(250, -170, expression(alpha == -170), cex = 1.5)
text(600, -70, expression(beta == 0.37), cex = 1.5)
```

`lm` 関数で計算した回帰オブジェクトを `summary` 関数に渡すと決定係数なども表示される．

```{r reg_summary}
summary(lm(price ~ weight, car))
```

## Estimation by hand

回帰モデル

$$ Y_i = \alpha + \beta X_i + u_i $$

の係数パラメタ（$\alpha, \beta$）を最小二乗法（OLS）によって求めよう．

最小二乗法とは，誤差 $u$ （残差 $\hat{u}_i = Y_i - \hat{Y}_i = Y_i - (\hat{\alpha}+\hat{\beta}X_i)$ で読み替えてもよい）の二乗和を最小化するように係数パラメタを求める方法である．

```{r error_in_scatter, echo = FALSE}
x <- c(1, 2, 3)
y <- c(1.2, 3.5, 2.0)  # 回帰直線から十分に離すよう調整
model <- lm(y ~ x)
y_hat <- predict(model)
plot(x, y, pch = 16, cex = 2, xlim = c(0.5, 3.5), ylim = c(0, 4), 
     xlab = "x", ylab = "y")
abline(model, col = "red", lwd = 2)
# 残差線
for (i in 1:length(x)) {
  segments(x[i], y[i], x[i], y_hat[i], col = "gray")
}
# Fitted values
points(x, y_hat, pch = 1, col = "red", cex = 2)
text(2.4, 3.1, expression(hat(u)[i] == Y[i] - hat(Y)[i]), cex = 1.5)
text(1.9, 1.8, expression(hat(Y)[i]), cex = 1.5)
text(1.9, 3.8, expression(Y[i]), cex = 1.5)
```

誤差の二乗和 $Q$ は以下で与えられる．

$$ Q = \sum_{i} u_i^2 = \sum_{i} (Y_i - \alpha - \beta X_i)^2 $$

<!--
以下のインタラクティブなアプレットで確認してみよう．

```{r shiny, echo = FALSE}
library(shiny) # すべてChatGPT 4oで作成
x_vals <- c(1, 2, 3)
y_vals <- c(1.2, 3.5, 2.0)
# plot(x_vals, y_vals); lm(y_vals ~ x_vals)
ui <- fluidPage(
  sidebarLayout(
    sidebarPanel(
      sliderInput("intercept", "Intercept (α):", min = -5, max = 5, value = 0, step = 0.1),
      sliderInput("slope", "Slope (β):", min = -5, max = 5, value = 1, step = 0.1)
    ),
    
    mainPanel(
      plotOutput("regPlot", height = "400px"),
    )
  )
)

server <- function(input, output) {
  output$regPlot <- renderPlot({
    alpha <- input$intercept
    beta <- input$slope
    y_hat <- alpha + beta * x_vals
    residuals <- y_vals - y_hat
    rss <- sum(residuals^2)
    
    plot(x_vals, y_vals, pch = 16, cex = 2, xlim = c(0, 4), ylim = c(0, 5),
         xlab = "X", ylab = "Y")
    text(.8, 4.5, paste0("Y = ", round(alpha,2), " + ", round(beta,2), " X"), cex = 2)
    text(.8, 3.8, paste0("Q = ", round(rss, 3)), cex = 2)
    abline(a = alpha, b = beta, col = "red", lwd = 2)
    segments(x_vals, y_vals, x_vals, y_hat, col = "grey")
  })
}

shinyApp(ui = ui, server = server)
```
-->

$Q$ は $\hat \alpha, \hat \beta$ のいずれについても下に凸の関数であるので，$Q$ をそれぞれで微分して 0 とおく．

$$ \frac{\partial Q}{\partial \alpha} = -2 \sum_i (Y_i - \alpha - \beta X_i) = 0 $$

$$ \frac{\partial Q}{\partial \beta} = -2 \sum_i X_i (Y_i - \alpha - \beta X_i) = 0 $$

上の連立方程式を解くことによって，最小二乗推定量は次のように与えられる．

$$ \hat{\alpha} = \bar{Y} - \hat{\beta} \bar{X}, \quad \hat{\beta} = \frac{S_{XY}}{S_{XX}} $$
$$ S_{XX} = \sum_i (X_i - \bar{X})^2, \quad S_{XY} = \sum_i (X_i - \bar{X}) (Y_i - \bar{Y}) $$

まず $\bar{Y}, \bar{X}, S_{XX}, S_{XY}$ という4つの統計量を計算する．

```{r reg_mean_s}
x <- car$weight
y <- car$price
y_bar <- mean(y)  # Y bar
x_bar <- mean(x)  # X bar
s_xx <- sum((x - x_bar)^2)  # S_XX
s_xy <- sum((x - x_bar) * (y - y_bar))  # S_XY
c(y_bar, x_bar, s_xx, s_xy)
```

計算した統計量を使って回帰係数を計算する．

```{r reg_coef}
beta_hat <- s_xy / s_xx
alpha_hat <- y_bar - beta_hat * x_bar
c(alpha_hat, beta_hat)
```

## Estimation by numerical calculation / 数値計算による推定

OLSは残差二乗和の最小化によって回帰係数パラメタを推定する方法（モデルをデータに当てはめる原理）であるが，これを解析的に解くのではなく数値計算によって解いてみる．

残差二乗和の関数（目的関数） $Q$ の最小化は R に built-in されている汎用の最適化関数 `optim` によって行う．

### Example of `optim` 

例：次の二次関数は $x=2$ で最小値 3 をとる．

$$ f(x) = x^2 -4x + 7 = (x-2)^2 + 3 $$

```{r fx, echo = FALSE}
plot(function (x) x^2-4*x+7, xlim = c(-2, 6))
points(2, 3, cex = 2, pch = 19, col = "red")
text(2, 4, labels = "(2, 3)")
```

最小値を取るような $x$ の値を `optim` 関数を使って求めてみる．

* `optim(par = パラメタの初期値ベクトル, fn = 目的関数)` の形で指定
   * `$par` ：最適化されたパラメタ（＝目的関数の引数；この例では $x$）
   * `$value` ：最小化された目的関数の値
   * `$convergence` ： 最適化計算が収束していれば 0

```{r optim_example}
obj_function <- function (x) x^2 - 4*x + 7
optim(par = 0, fn = obj_function, method = "BFGS")  # ニュートン法の亜種を利用
```


### In the case of regression 

回帰分析の場合，目的関数は残差二乗和 $Q$ であり，その引数は推定したい回帰係数パラメタ $(\alpha, \beta)$ となる．

$$ (\hat{\alpha}, \hat{\beta}) = \arg \min_{\alpha, \beta} Q(\alpha, \beta) $$
$$ Q (\alpha, \beta) := \sum_i \hat{u}_i^2 = \sum_i (Y_i - \alpha - \beta X_i)^2 $$

```{r reg_optim}
sum_of_squared_residuals <- function (coef_vector) {
  # coef_vector[1]: alpha (intercept)
  # coef_vector[2]: beta (slope of X)
  sum((y - coef_vector[1] - coef_vector[2] * x)^2)
}
sum_of_squared_residuals(coef_vector = c(0, 1))  # alpha=1, beta=2
sum_of_squared_residuals(coef_vector = c(0, 0.5))  # alpha=1, beta=3
optim(par = c(0, 0), fn = sum_of_squared_residuals)
```

$\hat{\alpha} = -170$ に固定して $\hat{\beta}$ の値によって残差二乗和がどのように変化するかをプロットすると，$\hat{\beta} = 0.37$ で最小化されていることが分かる．

<!--
（複雑なスクリプトなので，どのように動いているかは理解しなくてよい．）
-->

```{r reg_optim_given_beta0_plot, echo = FALSE}
sum_of_squared_residuals_beta <- function (beta1) {
  sum((y + 170 - beta1 * x)^2)
}
beta_value <- seq(0, 1, by = 0.01)
obj_fun <- sapply(beta_value, sum_of_squared_residuals_beta)
plot(x = beta_value, y = obj_fun, type = "l", xlim = c(0, 1), ylab = "Sum of squared errors", xlab = expression(beta))
points(0.365, 197529.7, pch = 19, cex = 2, col = "red")
text(0.365, 197529.7 + 3e6, label = "(0.37, 197529.7)")
```

## Coefficient of determination / 決定係数

$$ R^2 = \frac{\mbox{Variation in } Y \mbox{ explained by } X}{\mbox{Total variation in } Y} = \frac{S_{\hat{Y}\hat{Y}}}{S_{YY}} = 1 - \frac{\sum \hat{u}_i^2}{S_{YY}} $$

$$ S_{YY} = \sum_i (Y_i - \bar{Y})^2, \quad S_{\hat{Y} \hat{Y}} = \sum_i (\hat{Y}_i - \bar{\hat{Y}})^2 $$

```{r reg_r2}
s_yy <- sum((y - y_bar)^2)
y_pred <- alpha_hat + beta_hat * x  # y hat
s_yhyh <- sum((y_pred - y_bar)^2)
s_yhyh / s_yy  # R^2
res <- y - y_pred  # residuals
1 - sum(res^2) / s_yy  # R^2 ... should be the same as above
```

決定係数が 0.83 であることは「説明変数 (`weight`) によって被説明変数 (`price`) の変動のうち 83% を説明することができる」という意味．

余裕のある履修者向けの宿題：自由度調整済み決定係数を計算．

$$ \bar{R}^2 = 1 - \frac{n - 1}{n - K - 1} \frac{\sum \hat{u}_i^2}{S_{YY}} $$

$K$: 説明変数の数（定数項は含めない）

ヒント：$n$ は `nrow(car)` で与えられる（欠損値があればその分の調整も必要）．


## Standard error and test / 標準誤差と検定

帰無仮説「$H_0: \beta = 0$」の検定を行うために，$\hat{\beta}$ の標準誤差（s.e.）を求める．

$$ V(\hat{\beta}) = \frac{\sigma^2}{S_{XX}}, \quad s.e.(\hat{\beta}) = \sqrt{V(\hat{\beta})} $$

誤差分散 $\sigma^2$ は残差 $\hat{u}$ を利用して次のように推定する．

$$ \hat{\sigma}^2 = s^2 = \frac{1}{n - 2} \sum \hat{u}_i^2 $$

```{r reg_var}
n <- nrow(car)  # sample size 
sum(res^2) / (n - 2)  # estimated sigma^2
beta_se <- sqrt(sum(res^2)/(n - 2) / s_xx)  # standard error
beta_se
```

$$ t = \frac{\hat{\beta}}{\sqrt{V(\hat{\beta})}} $$

```{r reg_tvalue}
beta_hat / beta_se
```

$$ p = 2 \times [ 1 - \Pr(T \le |t|) ] $$

```{r pvalue_in_dist}
curve(dt(x, df = n - 2), from = -20, to = 20, main = "Density of t-distribution")
abline(v = beta_hat / beta_se * c(-1, 1), col = "red")
text(c(-15.1, 15.1), 0.02, labels = c("Pr(T < -15.1)", "Pr(T > 15.1)"))
2 * (1 - pt(q = beta_hat / beta_se, df = n - 2))
```

注：あまりにも小さいので 0 と表示されているが，厳密には 0 ではない．このような場合，論文・レポート等では「$<0.001$」のように書くことで，厳密には0ではないものの 0.1% 水準で有意であることが報告できる．


### Critical value approach / 臨界値を用いて検定を行う場合

t分布の2.5%臨界値（両側5%に対応）は？

```{r reg_t}
qt(p = 0.025, df = n - 2)  # df: degree of freedom (自由度)
qt(p = c(0.005, 0.025, 0.975, 0.995), df = n - 2)  # 1% and 5%
```

```{r pvalue_in_dist_cv}
curve(dt(x, df = n - 2), from = -4, to = 4, main = "Density of t-distribution")
abline(v = c(-2.01, 2.01), col = "red")
text(c(-3, 3), 0.04, labels = "2.5%"); text(0, 0.15, labels = "95%")
```

よって，$t = 15.1$ は有意水準1%に対応する臨界値 (2.7) よりも大きいので，1%水準で有意である．

自由度が大きければ標準正規分布の場合とほとんど同じ臨界値になる．

```{r qt_large_n}
c(qt(p = 0.975, df = 10000), qnorm(p = 0.975))
```

### Robust standard error / 頑健な標準誤差

詳しい議論はこの授業では行わないが，標準誤差は誤差項の不均一分散を前提として計算した方がよい．

`fixest` パッケージの `feols` 関数が便利．

```{r robust_se}
# install.packages("fixest")
library(fixest)
fixest::feols(price ~ weight + ps, car, vcov = "hetero")
summary(lm(price ~ weight + ps, car))$coef  # cf
```

同様にクラスター頑健標準誤差は次のように計算できる．

```{r cluster_robust_se}
fixest::feols(price ~ weight + ps, car, vcov = ~ maker)
```


# Multiple regression analysis / 重回帰分析

2つの説明変数をもつ次のような回帰モデル（重回帰モデル）を考える．

$$ \mbox{Price} = \alpha + \beta_1 \ \mbox{Weight} + \beta_2 \ \mbox{PS} + u $$

```{r multiple_reg_lm}
summary(lm(price ~ weight + ps, car))
```

`weight` の係数は「`ps` の水準を固定したもとで `weight` が1単位高い値を取ると `price` は 0.24 単位高い値を取る傾向にある」≒「馬力が同じ自動車同士で比較すると，車両が 1kg 重い自動車の価格は 2,400 円高い」と解釈される．

なお，「単回帰分析では因果関係が推定できないが，重回帰分析では因果関係が推定できる」という主張は一般には誤り．

- 重回帰分析によって因果効果を推定するためには，処置変数の条件付き独立（かなり大雑把に言うと，交絡変数がすべてコントロール変数として正確な関数形でモデルに含められていること）が成立していなければならず，この仮定を社会経済データを用いた実証分析において満たすことは容易ではない．


# Advanced method / 応用

## Dummy variables ダミー変数

`hybrid` 変数は，ハイブリッド車のときに 1 をとるダミー変数である．
ハイブリッド車以外であるガソリン車やディーゼル車で 0 をとる．

$$ \mbox{Price} = \alpha + \beta \ \mbox{Hybrid} + u $$

```{r reg_dummy_lm}
table(car$hybrid)
lm(price ~ hybrid, car)
```

この回帰係数 77 は「ハイブリッド車はハイブリッド車以外と比較して 77 万円高い」ことを意味している（ただし有意ではない）．

```{r reg_dummy_diff}
mean(car$price[car$hybrid == 0])
mean(car$price[car$hybrid == 1])
mean(car$price[car$hybrid == 1]) - mean(car$price[car$hybrid == 0])
```

下の図のように理解できるだろう．

「□」は `hybrid` の値ごとの平均価格を表している．
傾きは $\Delta y / \Delta x = (326 - 249) / 1 = 77$ である．

```{r reg_dummy_diff_plot, echo = FALSE}
plot(price ~ hybrid, car, cex = 1.5, xlim = c(-.2, 1.3), ylim = c(0, 800))
abline(lm(price ~ hybrid, car), col = "red")
abline(v = c(0, 1), col = "grey", lty = 2)
points(x = 0, y = mean(car$price[car$hybrid == 0]), col = "red", cex = 3, pch = 22)
points(x = 1, y = mean(car$price[car$hybrid == 1]), col = "red", cex = 3, pch = 22)
text(x = 0.2, y = 200, labels = expression(bar(Y) == 249), cex = 1.5)
text(x = 1.2, y = 270, labels = expression(bar(Y) == 326), cex = 1.5)
```


## Adding quardatic or interaction terms / 二次・交差項

$$ \mbox{Price} = \alpha + \beta_1 \mbox{PS} + \beta_2 \mbox{PS}^2 + u $$

<!--
新しい変数を作成するなどのデータフレームの操作をするためには `tidyverse` に含まれる関数 (`mutate`, `rename`, `group_by`, etc.) を用いると簡単．

# tidyverse パッケージを使用する場合
# library(tidyverse)
# swiss2 <- swiss %>%
#   mutate(Edu2 = Education^2,
#          ExamEdu = Examination * Education)

# swiss2$exam_dummy <- 1 * (swiss$Examination > mean(swiss$Examination))  # same as above 
# tidyverse パッケージを使用する場合
# swiss2 <- swiss %>%
#   mutate(exam_dummy = ifelse(Examination > mean(Examination), 1, 0))

# tidyverse パッケージを使用する場合
# swiss2 <- swiss %>% 
#   mutate(Exam_with_noise = Examination + rnorm(nrow(swiss), sd = 1))

# tidyverse パッケージを使用する場合
# swiss_scaled <- swiss %>% 
#   mutate(Fertility_scaled = (Fertility - mean(Fertility)) / sd(Fertility),
#          Examination_scaled = (Examination - mean(Examination)) / sd(Examination),
#          Agriculture_scaled = (Agriculture - mean(Agriculture)) / sd(Agriculture))
-->

```{r reg_mod, message = F}
car$ps2 <- car$ps^2
lm(price ~ ps + ps2, car)$coef
```

上の回帰式は次のような非線形な関係を表す．

```{r , echo = FALSE}
plot(price ~ ps, car, cex = 1.5)
xx <- seq(0, 500, by = 1)
yy <- predict(lm(price ~ ps + ps2, car), newdata = data.frame(ps = xx, ps2 = xx^2))
lines(xx, yy, col = "red")
```

データセットを直接操作せずに，回帰モデルを指定する際に `I` 関数を使って二乗項を表現することもできる．

```{r reg_quadratic_i}
lm(price ~ ps + I(ps^2), car)$coef
```

$$ \mbox{Price} = \alpha + \beta_1 \mbox{Weight} + \beta_2 \mbox{Hybrid} + \beta_3 \mbox{Weight} \times \mbox{Hybrid} + u $$

```{r reg_interaction}
car$wh <- car$weight * car$hybrid
lm(price ~ weight + hybrid + wh, car)$coef
```

このように，交差項を用いることでカテゴリー（この場合はハイブリッド車とガソリン車（・ディーゼル車））による回帰係数の異質性を表現できる．

$$ \mbox{Price} = -149 + 0.34 \mbox{Weight} - 168 \mbox{Hybrid} + 0.16 \mbox{Weight} \times \mbox{Hybrid} + u $$
$$ = \left\{ \begin{array}{l} -317 + 0.50 \mbox{Weight} + u \quad \mbox{if} \quad \mbox{Hybrid} \\ -149 + 0.34 \mbox{Weight} + u \quad \mbox{if} \quad \mbox{Not Hybrid} \end{array} \right. $$


「`var1 : var2`」は「`var1 × var2`」を表し，「`var1 * var2`」は「`var1 + var2 + (var1 × var2)`」を表す．

```{r reg_interaction_2}
lm(price ~ weight + hybrid + weight:hybrid, car)$coef
lm(price ~ weight*hybrid, car)$coef
```

## Log-linear model 

$$ \log(\mbox{Price}) = \alpha + \beta \ \mbox{Weight} + u $$

説明変数や被説明変数に対数を取る場合は `I` なしで `log` を使ってよい．

```{r reg_log}
lm(log(price) ~ weight, car)$coef
```

アウトカムに自然対数を取る場合，「$x$ が1単位大きな値を取る時に $y$ が $100 \times \beta$ % 高い値を取る」と解釈される．

$$ \beta = \frac{\partial \log(y)}{\partial x} = \frac{\partial \log(y)}{\partial y} \frac{\partial y}{\partial x} = \frac{1}{y} \frac{\partial y}{\partial x} = \frac{\partial y / y}{\partial x} $$

よって，「1 kg重い自動車の価格は 0.13 %高い」と解釈できる．

## Log-log model 

$$ \log(\mbox{Price}) = \alpha + \beta \ \log(\mbox{Weight}) + u $$

```{r reg_log_log}
lm(log(price) ~ log(weight), car)$coef
```

両辺に対数を取る場合は「$x$ が1%大きな値を取る時に $y$ が $\beta$ % 高い値を取る」と解釈される．

- $\beta$ は経済学では弾力性として知られる．

$$ \beta = \frac{\partial \log(y)}{\partial \log(x)} = \frac{\partial \log(y)}{\partial y} \frac{\partial x}{\partial \log(x)} \frac{\partial y}{\partial x} = \frac{x}{y} \frac{\partial y}{\partial x} = \frac{\partial y / y}{\partial x / x} $$

「1 %重い自動車の価格は 1.5 %高い」と解釈できる．


## Categorical variables 

複数の値を持つカテゴリー変数（質的変数）はダミー変数化する（すなわち量的変数に変換する）ことで回帰式に含めることができる．

いずれか一つのカテゴリーをベースラインとする（すべてのカテゴリーをダミー変数として含めると多重共線性により推定できなくなるため，一つは除外しなければならない）．

`maker` 変数はカテゴリー変数なので，これをダミー変数にしてみよう．

```{r maker_dummy}
table(car$maker)
car$Honda <- 1 * (car$maker == "Honda")
car$Mazda <- 1 * (car$maker == "Mazda")
car$Nissan <- 1 * (car$maker == "Nissan")
car$Subaru <- 1 * (car$maker == "Subaru")
car$Suzuki <- 1 * (car$maker == "Suzuki")
car$Toyota <- 1 * (car$maker == "Toyota")
lm(price ~ weight + Honda + Mazda + Nissan + Subaru + Suzuki + Toyota, car)$coef
```

たとえば，`Honda` の回帰係数 -17 は，ホンダの車がベースラインであるダイハツの車と比較して 17 万円低いことを表している（ただし統計的に有意ではない）．

`lm` 関数では，カテゴリー変数はそのまま回帰式に含めることで自動的にダミー変数として処理される．

```{r maker_dummy_2}
lm(price ~ weight + maker, car)$coef
```

### Change the reference group 

ベースラインのカテゴリーを変更するには `relevel` 関数を使う．

```{r category_dummy_base_honda}
car$maker <- relevel(as.factor(car$maker), ref = "Honda")
lm(price ~ weight + maker, car)$coef
```

今度は，ダイハツがベースラインのホンダと比較して 17 万円高いと解釈できる．


## Multicollinearity / 多重共線性

### Example of perfect multico. / 完全な多重共線性の例

```{r multico_perfect}
lm(price ~ weight + I(2 * weight), car)$coef
cor(car$weight, 2 * car$weight)
```

R では，該当する変数（のうち一つ）が自動的に drop される．


### Example of imperfect multico. / 不完全な多重共線性の例

互いに強い相関関係にある `weight`, `length`, `width` という3つの変数に着目しよう．
いずれの変数も `price` と正の相関を持ち，単回帰分析では正の回帰係数を得る．

```{r multico_imperfect}
cor(car[, c("price", "weight", "length", "width")])
summary(lm(price ~ weight, car))$coef
summary(lm(price ~ length, car))$coef
summary(lm(price ~ width, car))$coef
```

しかしながら，3つの変数を一つの回帰式に説明変数として同時に含めると，いずれの変数も標準誤差が大きくなる．
ラフに言えば，これは推定が安定しなくなることを意味する．
実際，`width` の回帰係数は有意に負の値をとっているが，これは解釈しにくい結果である．

```{r multico_imperfect2}
summary(lm(price ~ weight + length + width, car))$coef
```

### Using VIF for diagnostics / VIF による診断 

変数 $j$ の分散拡大係数（variance inflation factor, VIF）は，変数 $j$ をそれ以外の変数に回帰したときの決定係数 $R_j^2$ を使って以下のように計算される．

$$ VIF_j = \frac{1}{1 - R_j^2} $$

経験則として「$VIF > 10$ ならば多重共線性が生じている」と判断されることが多い．

たとえば $j =$ `length` としてこの変数のVIFを計算してみよう．

```{r vif_manual}
lm_length <- lm(length ~ weight + width, car)
r2_length <- summary(lm_length)$r.squared  # cf. str(summary(lm_length))
1 / (1 - r2_length)  # VIF 
```

`car` パッケージの `vif` 関数を使うと便利．
なお，このパッケージ名がデータセットを格納しているオブジェクト名と同じなので少しだけややこしいが，パッケージの方の名称は An R **C**ompanion to **A**pplied **R**egression という書籍名から来ている．

```{r vif_car, message = FALSE}
# install.packages("car")
library(car)
car::vif(lm(price ~ weight + length + width, car))
```

多重共線性への対処：

- 深刻な多重共線性が生じている場合は該当する変数のうち（少なくとも）一つをモデルから外すことがある．とりわけ予測モデルとして回帰モデルを推定する際は変数の除去は珍しくない．
- 一方で key variable $X_1$ がアウトカムに与える因果効果を測ろうとして回帰モデルを推定する場合に，$X_1$ と相関がある（かつアウトカムに影響を与える）変数 $X_2$ を除外すると除外変数バイアスが生じる可能性があるため，$X_2$ は除外するべきではないだろう．
    - ただし，$X_2$ が $X_1$ の影響を受ける変数でありかつアウトカムに影響を与える変数である場合に，$X_1$ がアウトカムに与える効果（$X_1$ から直接与える効果と $X_2$ を経由して間接的に与える効果の両方を合計したもの）を推定したい場合は，むしろ $X_2$ は除外するべきである．これは多重共線性とは別の理由による（いわゆる bad control の問題）．


## Comparing the size of the coefficients / 回帰係数の大きさを比較する

例：`weight` と `ps` の回帰係数の（絶対値の）大きさを比較する．

```{r car_comparison_naive}
lm(price ~ weight + ps, car)$coef
```

上の結果から「$|0.24| < |0.78|$ なので `ps` の方が `weight` の分散をより説明する」と結論付けることはできない．

なぜなら，係数の大きさは変数のスケール（ばらつき）に依存するから．

たとえば，線形モデル $y = \beta_0 + \beta_1 x + u$ において金額ベースの変数 $x$ の単位を「円」から「千円」に変えると，推定される値そのもの ($\hat{\beta}_1$) は1千倍になるが，推定された回帰係数の「解釈」は全く変わらない（この変数の重要性が1千倍になるわけではない）．


### Comparing the change per one SD / 1標準偏差当たりの変化量の比較

```{r car_comparison_sd}
c(0.24 * sd(car$weight), 0.78 * sd(car$ps, na.rm = TRUE))
```

「$|90.8| > |58.7|$ なので `weight` の方が `price` の分散に対する説明力がより高い」と言える．


### Standardized partial regression coefficient / 標準偏回帰係数

より一般的な方法は，標準化（平均 0，分散 1 になるように変換）した変数を使って回帰分析をするもの．

```{r car_comparison_scaled}
car$price_s <- (car$price - mean(car$price)) / sd(car$price)
car$weight_s <- (car$weight - mean(car$weight)) / sd(car$weight)
car$ps_s <- (car$ps - mean(car$ps, na.rm = TRUE)) / sd(car$ps, na.rm = TRUE)
lm(price_s ~ weight_s + ps_s, car)$coef
```

「$|0.59| > |0.38|$ なので `weight` の方が `price` の分散をより説明する」と言える．

なお，t 値と p 値は標準化の前後で不変．

スケーリング（分散を 1 にする）とセンタリング（平均を 0 にする）を同時に行ってくれる `scale` という関数を使えばより簡単．

```{r car_comparison_scale_func, eval = FALSE}
lm(price ~ weight + ps, as.data.frame(scale(car[, c(-1, -2)])))
```

（出力は省略）


## Prediction / 予測

当てはめ値は定義通り次のように計算できる．

```{r pred_hand}
lm(price ~ weight + ps, car)$coef
car$pred0 <- -103.71 + 0.2368 * car$weight + 0.7765 * car$ps
head(car[, c("price", "pred0")])
```

たとえば {1,000 kg, 100 ps} という仮想的な車種の価格を予測するには，上の計算式にこの属性値を代入すればよい．

```{r pred_hand2}
-103.71 + 0.2368 * 1000 + 0.7765 * 100
```

推定に用いたサンプルの当てはめ値も，out-of-sample の予測も，`predict` 関数で計算できる．

- 予測値を元のデータセットにマージしたい場合，変数に欠損値があると予測値ベクトルのサイズが元のデータセットの行数より小さくなってしまうため，次のように `na.action = na.exclude` オプションを追加する．

```{r pred}
lm1 <- lm(price ~ weight + ps, car, na.action = na.exclude)
car$pred1 <- predict(lm1, newdata = car)
head(car[, c("price", "pred0", "pred1")])
predict(lm1, newdata = data.frame(weight = 1000, ps = 100))
```

Log-linear model の場合，予測値を元の変数のスケールに戻すには次のようにする．

$$ \hat{y} = \exp \left( \hat{\log{y}} + \frac{\sigma^2}{2} \right) $$

- 当てはめた値は $\hat{\log{y}}$ であって $\log{\hat{y}}$ ではないことに注意．
- $\sigma^2$ は誤差分散．残差の分散で代用する．

```{r log_linear_exp}
lm2 <- lm(log(price) ~ weight + ps, car, na.action = na.exclude)
sigma2 <- summary(lm2)$sigma^2
car$pred2 <- exp(predict(lm2, newdata = car) + sigma2/2)
head(car[, c("price", "pred0", "pred1", "pred2")])
```


## Model selection / モデル選択

被説明変数の予測を目的として回帰分析を行う場合，複数のモデル候補があった場合に，予測力が最も優れたモデルを一つ選択したい場合があるだろう．
しかし，説明変数を追加すれば決定係数は必ず増加するが，それは標本に特有のデータの変動（あるいはノイズ）に過適合しているだけかもしれない．
この過適合によって説明変数過剰のモデルが選択されることを防ぐために，モデルの説明力とモデルの簡潔さのトレードオフの間でバランスを取るための何らかの指標が必要となる．

実務的に広く利用されるのはAIC（赤池情報量規準）と呼ばれる規準．
これはモデルを最尤推定することを前提として「最尤推定量の対数尤度」と「パラメタの数（＝定数項を含む説明変数の数＝K+1）」のトレードオフとして記述される．

$$ AIC = -2 \log L + 2(K+1) $$

対数尤度は大きいほどモデルのデータへの当てはまりが良く，principle of parsimony の観点からパラメタ数は少ない方がよい．
すなわち，複数のモデル間で AIC が最も小さいモデルが望ましい．

注：

* 因果推論を目的として回帰分析を行う場合には一般にモデル選択は行わない場合が多い．分析の目的に応じてモデル選択を行うかどうかを決めよう．
  * 典型的な AIC の使いどころは時系列分析におけるラグの次数決定．
* AICは「真のモデル」を選択する基準ではない．あくまで予測力の観点で（比較対象の中で相対的に最も）望ましいモデルを選択する基準である．
* 「AICは入れ子関係のモデル間の比較にしか使えない」という主張に稀に出くわすが，それは誤り．入れ子関係のモデル間でないと適用できない一部の検定と混同されているものと思われる．詳しくは Burnham and Anderson (2002) [Model Selection and Multimodel Inference](https://link.springer.com/book/10.1007/b97636) (Ch. 2) などを参照．
* モデル選択後に回帰係数の検定は行わない．少なくとも，オーソドックスな t 検定をナイーブに適用するのはNG．

<!--
MODEL SELECTION AND INFERENCE: FACTS AND FICTION
https://www.cambridge.org/core/journals/econometric-theory/article/abs/model-selection-and-inference-facts-and-fiction/EF3C7D79D5AFC4C6325345A3C8E26296
-->

### Find AIC 

データセットが欠損値を含むため `na.omit` 関数を使って欠損値を含む行を予めすべて削除する．

```{r aic}
lm1 <- lm(price ~ weight, na.omit(car))
lm2 <- lm(price ~ weight + ps, na.omit(car))
lm3 <- lm(price ~ weight + ps + length + width, na.omit(car))
AIC(lm1, lm2, lm3)
```

2つ目のモデルの値が最も小さいが，2つ目と3つ目の差は十分に小さい（2を下回っている）ため，2つ目と3つ目のモデルはほとんど同じ予測力を持つと判断できる．

### Stepwise model selection with AIC

考え得る説明変数をすべて含めたモデル (full model) からスタートして stepwise で最適なモデルを選択することもできる．

`step` 関数を使うと AIC を規準としてモデル選択が行われる．

```{r aic_step}
lm_full <- lm(price ~ weight + length + width + height + WLTC + 
                ps + disp + hybrid + accel + kei + maker, na.omit(car))
lm_step <- step(lm_full, trace = 0)  # trace = 1 にすると計算過程が出力される
summary(lm_step)$coef
AIC(lm_step)
```

しかし符号が直感と反する変数が幾つかある．
VIFをチェックすると案の定多重共線性が生じている．

```{r vif_after_step}
car::vif(lm_step)
```

### Cross validation / 交差検証

回帰モデルの予測性能（汎化性能）を評価するための手法として交差検証はよく用いられる．
交差検証ではデータを「推定用」と「評価用」に分割し，推定用のサブサンプルで推定されたモデルの性能を評価用のサブサンプル（out-of-sample）で評価する．

- 機械学習の文脈では訓練データ・検証データ・テストデータと区別するが，ここでは検証データで最終評価を行う（テストデータのように扱う）場合を考える．

評価指標には MSE (mean squared error) などが用いられる．
MSE の標本対応は次の通り．
$\hat{Y}_i$ は $i$ 以外のデータ（すなわち $1,2,\ldots,i-2,i-1,i+1,i+2,\ldots,n$）によって推定されたモデルのパラメタ（$\hat{\alpha}_{\ne i}, \hat{\beta}_{\ne i}$）に基づいて予測された $i$ の値である．

$$ \mbox{MSE} = \frac{1}{n} \sum_{i = 1}^n (Y_i - \hat{Y}_i)^2 \quad \mbox{where} \quad \hat{Y}_i = \hat{\alpha}_{\ne i} + \hat{\beta}_{\ne i} \ X_i $$

この演習で使っている `car` データはサンプルサイズが小さいため，評価用のサブサンプルはサイズ1としよう．
つまり，以下のように Step 1, Step 2, ..., Step n を実行し，二乗誤差の平均値（あるいは中央値）を計算する．

- Step 1. $i=2,\ldots,n$ を推定用，$i=1$ を評価用として，二乗誤差を計算
- Step 2. $i=1,3,\ldots,n$ を推定用，$i=2$ を評価用として，二乗誤差を計算
- Step 3. $i=1,2,4,\ldots,n$ を推定用，$i=3$ を評価用として，二乗誤差を計算
- Step 4. $i=1,2,3,5,\ldots,n$ を推定用，$i=4$ を評価用として，二乗誤差を計算
- ...
- Step n. $i=1,\ldots,n-1$ を推定用，$i=n$ を評価用として，二乗誤差を計算

このような交差検証を Leave-One-Out CV と呼ぶ．

説明変数が少ないモデル（alternative model）の方が予測誤差が小さい．
これは full model で生じている深刻な多重共線性が幾つかの変数削除によって解消されたため（そして，それによる予測力の向上が説明変数を減らすことによる説明力の低下を相殺してなお上回るため）であると考えられる．

```{r cv}
car2 <- na.omit(car)
se <- NULL
for (i in 1:nrow(car2)) {
  # full model 
  lm_full_i <- lm(price ~ weight + length + width + height + WLTC + 
                    ps + disp + hybrid + accel + kei + year, car2[-i,])
  pred_full_i <- as.numeric(predict(lm_full_i, newdata = car2[i, ]))  # hat{y}_i
  se_full_i <- (car2$price[i] - pred_full_i)^2  # (squared error)_i
  # alternative model 
  lm_alt_i <- lm(price ~ weight + height + ps + hybrid + kei + year, car2[-i,])
  pred_alt_i <- as.numeric(predict(lm_alt_i, newdata = car2[i, ]))
  se_alt_i <- (car2$price[i] - pred_alt_i)^2
  # stack
  se <- rbind(se, c(id = i, price = car2$price[i], 
                    pred_full = pred_full_i, se_full = se_full_i, 
                    pred_alt = pred_alt_i, se_alt = se_alt_i))
}
summary(se[, c(4, 6)])
# plot(pred_full ~ price, se, cex = 1.5); abline(a = 0, b = 1, col = "red")
# plot(pred_alt ~ price, se, cex = 1.5); abline(a = 0, b = 1, col = "red")
```

MSE が小さい方のモデルは以下のように推定されている．

```{r min_mse}
lm_alt <- lm(price ~ weight + height + ps + hybrid + kei + year, car2)
summary(lm_alt)$coef
car::vif(lm_alt)
```

これでもなお不安な符号が幾つか残っているが（全高 `height` が負，軽自動車ダミー `kei` が正），何らかの除外変数によるものなのか，あるいはこの符号で実際に正しいのかは不明．


# Take home messages 

* 回帰分析：`summary(lm(formula = Y ~ X1 + X2, data = dataset))` 
* 説明変数の二乗は `I(X1^2)` 
* 2つの説明変数の掛け算（交差項）は `X1:X2`

.
